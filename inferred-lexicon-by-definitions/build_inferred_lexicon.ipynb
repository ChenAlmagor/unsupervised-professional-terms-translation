{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "TnNOw7yE0oen",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!pip uninstall --quiet --yes tensorflow\n",
    "!pip install --quiet tensorflow-gpu==1.13.1\n",
    "!pip install --quiet tensorflow-hub\n",
    "!pip install --quiet seaborn\n",
    "!pip install --quiet tf-sentencepiece\n",
    "!pip install --quiet simpleneighbors\n",
    "!pip install --quiet tqdm\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tf_sentencepiece  # Not used directly but needed to import TF ops.\n",
    "\n",
    "from simpleneighbors import SimpleNeighbors\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "n3ShSe2Gpftj",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/1'  \n",
    "\n",
    "# Set up graph.\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "    multiling_embed = hub.Module(module_url)\n",
    "    embedded_text = multiling_embed(text_input)\n",
    "    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "g.finalize()\n",
    "\n",
    "# Initialize session.\n",
    "session = tf.Session(graph=g)\n",
    "session.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "nt_tqsb5w49V",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "corpus_metadata = [\n",
    "    ('en', '/content/test-en.csv', 'English'),\n",
    "    ('es', '/content/test-es.csv', 'Spanish'),\n",
    "]\n",
    "\n",
    "language_to_sentences = {}\n",
    "language_to_terms = {}\n",
    "\n",
    "\n",
    "en_csv = pd.read_csv('./test-en.csv')\n",
    "es_csv = pd.read_csv('./test-es.csv')\n",
    "language_to_sentences['en'] = en_csv['Definitions']\n",
    "language_to_sentences['es'] = es_csv['Definitions']\n",
    "language_to_terms['en'] = en_csv['Terms']\n",
    "language_to_terms['es'] = es_csv['Terms']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "UFTG9fuKgVvP",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "## Using a pre-trained model to transform sentences into vectors\n",
    "import sys\n",
    "\n",
    "batch_size = 512\n",
    "language_to_embeddings = {}\n",
    "\n",
    "for language_code, terms_file, language_name in corpus_metadata:\n",
    "    print('\\nComputing {} embeddings'.format(language_name))\n",
    "    with tqdm(total=len(language_to_sentences[language_code])) as pbar:\n",
    "        for batch in pd.read_csv(terms_file, chunksize=batch_size):\n",
    "            try:\n",
    "                 language_to_embeddings.setdefault(language_code, []).extend(\n",
    "                    session.run(embedded_text, feed_dict={text_input: batch['Definitions']}))\n",
    "            except:\n",
    "                  print (\"Unexpected error:\", sys.exc_info()[0])\n",
    "                  raise\n",
    "\n",
    "            pbar.update(len(batch))\n",
    "            \n",
    "            \n",
    "language_to_embeddings['en'] = np.array(language_to_embeddings['en'])\n",
    "emb_en = language_to_embeddings['en'] \n",
    "language_to_embeddings['es'] = np.array(language_to_embeddings['es'])\n",
    "emb_es = language_to_embeddings['es'] \n",
    "\n",
    "\n",
    "np.save('emb_en', emb_en)\n",
    "np.save('emb_es', emb_es)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "BnvE4-kn1sqY",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "emb_en = np.load('/content/emb_en.npy')\n",
    "emb_es = np.load('/content/emb_es.npy')\n",
    "\n",
    "\n",
    "language_to_embeddings['en'] = emb_en\n",
    "language_to_embeddings['es'] = emb_es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "D5wHfk2_L-dC",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "## Building an index of semantic vectors\n",
    "\n",
    "\n",
    "num_index_trees = 40\n",
    "language_name_to_index = {}\n",
    "embedding_dimensions = len(list(language_to_embeddings.values())[0][0])\n",
    "\n",
    "num_index_trees = 60\n",
    "print('Computing mixed-language index')\n",
    "combined_index = SimpleNeighbors(embedding_dimensions, metric='dot')\n",
    "for language_code, terms_file, language_name in corpus_metadata:\n",
    "    print('Adding {} embeddings to mixed-language index'.format(language_name))\n",
    "    for i in trange(len(language_to_sentences[language_code])):\n",
    "        annotated_sentence = '({}) {}'.format(language_name, language_to_sentences[language_code][i])\n",
    "        combined_index.add_one(annotated_sentence, language_to_embeddings[language_code][i])\n",
    "\n",
    "print('Building mixed-language index with {} trees...'.format(num_index_trees))\n",
    "combined_index.build(n=num_index_trees)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0jO1u41qjAxR",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#Match EN-ES\n",
    "def find_parallel_nn(search_results):\n",
    "  for result in search_results:\n",
    "    if \"(Spanish)\" in result:\n",
    "      return result\n",
    "  raise 'NO PARALLEL'\n",
    "    \n",
    "def find_term_and_sentence_in_corpus(sentence):\n",
    "  for i in range(len(language_to_sentences['es'])):\n",
    "    if sentence.endswith(language_to_sentences['es'][i]):\n",
    "      return (i, language_to_terms['es'][i], language_to_sentences['es'][i])\n",
    "  \n",
    "df_en_es = pd.pandas.DataFrame(columns=['en_es_index_es', 'en_es_index_en','en_es_term_es', 'en_es_term_en', 'en_es_def_es', 'en_es_def_en'])\n",
    "\n",
    "en_dict = pd.read_csv('./test-en.csv')\n",
    "\n",
    "\n",
    "with tqdm(total=len(language_to_sentences['en'])) as pbar:\n",
    "  for row in en_dict.iterrows():\n",
    "    index_en = row[0]\n",
    "    data = row[1]\n",
    "    term_en = data['Terms']\n",
    "    definition_en = data['Definitions']\n",
    "\n",
    "    sample_query = definition_en\n",
    "    num_results = 1500  \n",
    "    query_embedding = session.run(embedded_text, feed_dict={text_input: [sample_query]})[0]\n",
    "    transformed_query_embedding = en_es_T.dot(query_embedding.T).T\n",
    "    search_results = combined_index.nearest(transformed_query_embedding, n=num_results)\n",
    "\n",
    "    try:\n",
    "      parallel_nn = find_parallel_nn(search_results)\n",
    "      (index_es, term_es, definition_es) = find_term_and_sentence_in_corpus(parallel_nn)\n",
    "      df_en_es = df_en_es.append({'en_es_index_es':index_es, 'en_es_index_en':index_en,'en_es_term_es': term_es, 'en_es_term_en':term_en, 'en_es_def_es':definition_es, 'en_es_def_en':definition_en}, ignore_index=True)\n",
    "    except:\n",
    "      print('####### ERROR ##########')\n",
    "      print (\"Unexpected error:\", sys.exc_info()[0])\n",
    "      print (definition_en)\n",
    "      print(search_results)\n",
    "      raise\n",
    "    pbar.update(1)\n",
    "\n",
    "df_en_es.to_csv('match-en-es.csv')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "TIgzkdr_6g0r",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#Match ES-EN\n",
    "\n",
    "def find_parallel_nn_es_en(search_results):\n",
    "  for result in search_results:\n",
    "    if \"(English)\" in result:\n",
    "      return result\n",
    "  raise 'NO PARALLEL'\n",
    "    \n",
    "def find_term_and_sentence_in_corpus_es_en(sentence):\n",
    "  for i in range(len(language_to_sentences['en'])):\n",
    "    if sentence.endswith(language_to_sentences['en'][i]):\n",
    "      return (i, language_to_terms['en'][i], language_to_sentences['en'][i])\n",
    "  \n",
    "df_es_en = pd.pandas.DataFrame(columns=['es_en_index_es', 'es_en_index_en','es_en_term_es', 'es_en_term_en', 'es_en_def_es', 'es_en_def_en'])\n",
    "\n",
    "es_dict = pd.read_csv('./test-es.csv')\n",
    "with tqdm(total=len(language_to_sentences['es'])) as pbar:\n",
    "  for row in es_dict.iterrows():\n",
    "    index_es = row[0]\n",
    "    data = row[1]\n",
    "    term_es = data['Terms']\n",
    "    definition_es = data['Definitions']\n",
    "\n",
    "    sample_query = definition_es\n",
    "    num_results = 1500  \n",
    "    query_embedding = session.run(embedded_text, feed_dict={text_input: [sample_query]})[0]\n",
    "    transformed_query_embedding = es_en_T.dot(query_embedding.T).T\n",
    "    search_results = combined_index.nearest(transformed_query_embedding, n=num_results)\n",
    "\n",
    "    try:\n",
    "      parallel_nn = find_parallel_nn_es_en(search_results)\n",
    "      (index_en, term_en, definition_en) = find_term_and_sentence_in_corpus_es_en(parallel_nn)\n",
    "      #print({'es_en_index_es':index_es, 'es_en_index_en':index_en,'es_en_term_es': term_es, 'es_en_term_en':term_en, 'es_en_def_es':definition_es, 'es_en_def_en':definition_en})\n",
    "      df_es_en = df_es_en.append({'es_en_index_es':index_es, 'es_en_index_en':index_en,'es_en_term_es': term_es, 'es_en_term_en':term_en, 'es_en_def_es':definition_es, 'es_en_def_en':definition_en}, ignore_index=True)\n",
    "      \n",
    "    except:\n",
    "      print('####### ERROR ##########')\n",
    "      print (\"Unexpected error:\", sys.exc_info()[0])\n",
    "\n",
    "      print (definition_en)\n",
    "      print(search_results)\n",
    "    pbar.update(1)\n",
    "\n",
    "df_es_en.to_csv('match-es-en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "t6lvkyQq2mvF",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "df_es_en = pd.read_csv('/content/match-es-en.csv')\n",
    "df_en_es = pd.read_csv('/content/match-en-es.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Jo06MWhj8H0Y",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "df_intersection = pd.pandas.DataFrame(columns=['term_en', 'term_es', 'en_es_index_es', 'en_es_index_en','en_es_term_es', 'en_es_term_en', 'en_es_def_es', 'en_es_def_en', 'es_en_index_es', 'es_en_index_en','es_en_term_es', 'es_en_term_en', 'es_en_def_es', 'es_en_def_en' ])\n",
    "\n",
    "\n",
    "embeddings_en = []\n",
    "embeddings_es = []\n",
    "for row_es in df_es_en.iterrows():\n",
    "    index = row_es[0]\n",
    "    data = row_es[1]\n",
    "\n",
    "\n",
    "    es_en_index_es = int(data['es_en_index_es'])\n",
    "    es_en_index_en = int(data['es_en_index_en'])\n",
    "    es_en_term_es = data['es_en_term_es']\n",
    "    es_en_term_en = data['es_en_term_en']\n",
    "    es_en_def_es = data['es_en_def_es']\n",
    "    es_en_def_en = data['es_en_def_en']\n",
    "\n",
    "\n",
    "\n",
    "    row_en = df_en_es.iloc[[es_en_index_en]]\n",
    "\n",
    "\n",
    "    en_es_index_es = int(row_en['en_es_index_es']) \n",
    "    en_es_index_en = int(row_en['en_es_index_en']) \n",
    "    en_es_term_es = row_en['en_es_term_es']\n",
    "    en_es_term_en = row_en['en_es_term_en']\n",
    "    en_es_def_es = row_en['en_es_def_es']\n",
    "    en_es_def_en = row_en['en_es_def_en']\n",
    "\n",
    "\n",
    "    term_en = str(es_en_term_en)\n",
    "    term_es = str(es_en_term_es)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if (en_es_index_es == es_en_index_es):\n",
    "\n",
    "        def_en = es_en_def_en\n",
    "        def_es = es_en_def_es\n",
    "\n",
    "        annotated_sentence_en = '({}) {}'.format('English' ,def_en)\n",
    "        annotated_sentence_es = '({}) {}'.format('Spanish' ,def_es)\n",
    "        np.set_printoptions(threshold=sys.maxsize)\n",
    "        vec_def_en =   combined_index.vec(annotated_sentence_en)\n",
    "        vec_def_es =   combined_index.vec(annotated_sentence_es)\n",
    "\n",
    "\n",
    "        embeddings_en.append(vec_def_en)\n",
    "        embeddings_es.append(vec_def_es)\n",
    "        df_intersection = df_intersection.append({'term_en': term_en, 'term_es': term_es, 'es_en_index_es':es_en_index_es, 'es_en_index_en':es_en_index_en,'es_en_term_es': es_en_term_es, 'es_en_term_en':es_en_term_en, 'es_en_def_es':es_en_def_es, 'es_en_def_en':es_en_def_en, 'en_es_index_es':en_es_index_es, 'en_es_index_en':en_es_index_en,'en_es_term_es': en_es_term_es, 'en_es_term_en':en_es_term_en, 'en_es_def_es':en_es_def_es, 'en_es_def_en':en_es_def_en}, ignore_index=True)\n",
    "\n",
    "\n",
    "df_intersection.to_csv('intersection.csv')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "build-inferred-lexicon.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
